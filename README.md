# Preparation
```
conda create -n deeporacle python=3.9
pip install -r requirements.txt
```


## Install Defects4J
- See: https://github.com/rjust/defects4j
- Install v2.0.0

## Install GrowingBugs
- See: https://github.com/liuhuigmail/GrowingBugRepository
- Install v7.0
- Commit a880ccb, adding 100 new bugs


# Directory Architecture
```text
.
â”œâ”€â”€ data
â”œâ”€â”€ eval
â”‚Â Â  â”œâ”€â”€ rq1.py
â”‚Â Â  â””â”€â”€ rq3.py
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ RQ1
â”œâ”€â”€ RQ2
â”œâ”€â”€ RQ3
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ ablation.py
â”‚Â Â  â”œâ”€â”€ baseline_llm.py
â”‚Â Â  â”œâ”€â”€ exception_judge.py
â”‚Â Â  â”œâ”€â”€ gen_oracle.py
â”‚Â Â  â”œâ”€â”€ gen_prefix_by_llm
â”‚Â Â  â”œâ”€â”€ get_scenario.py
â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”œâ”€â”€ postprocess.py
â”‚Â Â  â”œâ”€â”€ processing.py
â”‚Â Â  â”œâ”€â”€ __pycache__
â”‚Â Â  â”œâ”€â”€ template
â”‚Â Â  â”œâ”€â”€ test.ipynb
â”‚Â Â  â””â”€â”€ voter.py
â””â”€â”€ structure.txt
```

# Use DeepOracle to Generate oracles 
requires configuring API-KEY (`src/.env`)
```bash
cd src
# Generate Scenario
python get_scenario.py work_dir
# Generate oracle candidates and vote
python gen_oracle.py work_dir
python voter.py work_dir
# Execute exception Inference 
python exception_judge.py work_dir
```
* `work_dir` represents the working directory, which should include inputs.csv, meta.csv, context.csv

* `inputs.csv`:
Contains the focal method, test prefixes, and related input information used for generation.

* `meta.csv`:
Stores metadata of the test prefixes, such as test_name and project_id used during generation and analysis.

* `context.csv`:
Stores the class-level context of the focal method.
---

# Reproduction
## RQ1
### Executing oracles generated by different methods
* Datasets need to be prepared before running
* All runtime data and results during the experiment are stored in `data/run_record.tar.gz`
``` bash
# generate oracles using DeepOracle and Baselines
tar -xzvf data/run_record.tar.gz -C data
tar -xzvf RQ1/data/test_framework.tar.gz -C RQ1/data
cd RQ1
# run experiments for rq1_1 and rq1_2
bash run_rq1_2.sh 4 1
```

### Display the comparison results with baselines
* Alternatively, you can directly read our recorded run results from `data/run_record.tar.gz`
* You can execute the following script to view the data statistics results.
``` bash
tar -xzvf data/run_record.tar.gz -C data
cd eval
python rq1.py
```
* Execution result
```
RQ1_1 Results: EvoSuite-Generated Test Prefixes
Defects4J
Method          BugFound         FPR   Precision    TP    FP    TN
toga                  46      23.98%      35.60%    68   123   390
togll                 18      43.77%      12.77%    36   246   316
llm_direct            42      25.29%      32.83%    65   133   393
deeporacle            53      21.56%      39.58%    76   116   422

GrowingBugs
Method          BugFound         FPR   Precision    TP    FP    TN
toga                   5      23.23%      20.69%     6    23    76
togll                  1      37.21%      31.91%    15    32    54
llm_direct             4      32.18%      30.00%    12    28    59
deeporacle             8      26.67%      38.46%    15    24    66

RQ1_2 Results: LLM-Generated Test Prefixes
Defects4J
Method          BugFound         FPR   Precision    TP    FP    TN
toga                  59      13.98%      27.92%   110   284  1747
togll                 34      30.77%       5.86%    41   659  1483
llm_direct           120      18.50%      38.82%   217   342  1507
deeporacle           138      18.00%      43.79%   261   335  1526

GrowingBugs
Method          BugFound         FPR   Precision    TP    FP    TN
toga                   7      13.21%      34.38%    11    21   138
togll                  2      26.32%       8.16%     4    45   126
llm_direct            10      25.52%      37.29%    22    37   108
deeporacle            15      23.29%      46.88%    30    34   112
```

ðŸ“¦ Structure of `run_record.tar.gz`
``` bash
.
â”œâ”€â”€ bugfound_evo.csv
â”œâ”€â”€ bugfound_llm.csv
â”œâ”€â”€ d4j_evo_prefix
â”œâ”€â”€ d4j_llm_prefix
â”œâ”€â”€ gb_evo_prefix
â””â”€â”€ gb_llm_prefix
```
### Top-level files

* `bugfound_evo.csv` and `bugfound_llm.csv`
These two files summarize the bugs found by different approaches on two types of test prefixes (evosuite-based and LLM-based, respectively).


### Top-level directories
The four directories:
* `d4j_evo_prefix`
* `d4j_llm_prefix`
* `gb_evo_prefix`
* `gb_llm_prefix`

correspond to different datasets (e.g., Defects4J and GrowingBugs) and different types of generated test prefixes (evosuite-based vs. LLM-based).

Each of these directories contains four subdirectories:

* `deeporacle` (our approach)
* `llm_direct` (baseline)
* `toga` (baseline)
* `togll` (baseline)

Each subdirectory represents the results produced by a specific method on the corresponding dataset and prefix type.

### Files inside each method directory

Within each method directory (e.g., deeporacle/), the main files and folders include:
* `context.csv`:
Stores the class-level context of the focal method.

* `inputs.csv`:
Contains the focal method, test prefixes, and related input information used for generation.

* `scenarios.csv`:
Stores the generated test scenarios.

* `test_case_llm_v1.csv`, `test_case_llm_v2.csv`, `test_case_llm_v3.csv`:
Store the complete generated test cases with oracles from three independent runs.

* `*_generated/` (e.g., `togs_generated/`):
Contains various intermediate records produced during test case execution.

* `rq1.csv`:
Stores the execution results of the generated test cases, used for answering RQ1.


## RQ2
Test Scenario Evaluation samples and evaluation results are stored in `RQ2`


## RQ3
### Display the ablation experiment results
All runtime data and results during the experiment are stored in `RQ3/ablation_run_record.tar.gz`.

You can execute the following script to view the data statistics results.
``` bash
tar -xzvf RQ3/ablation_run_record.tar.gz -C RQ3
cd eval
python rq3.py
```
* Execution result
```
RQ3 Results: Ablation Study
EvoSuite-Generated Test Prefixes
Method                     BugFound         FPR   Precision    TP    FP    TN
deeporacle                       61      22.29%      39.39%    91   140   488
without_exception_inf            56      25.96%      34.80%    87   163   465
without_scenario_inf             43      25.20%      35.00%    84   156   463
without_both                     41      26.66%      32.93%    81   165   454

LLM-Generated Test Prefixes
Method                     BugFound         FPR   Precision    TP    FP    TN
deeporacle                      153      18.39%      44.09%   291   369  1638
without_exception_inf           151      19.62%      42.31%   289   394  1614
without_scenario_inf            142      22.77%      35.76%   260   467  1584
without_both                    140      23.94%      34.53%   259   491  1560
```

### Manually rerun the ablation experiment 
requires configuring API-KEY (`src/.env`)

* Without Exception Inference
``` bash
# Generate Scenario
python get_scenario.py work_dir
# Generate oracle candidates and vote
python gen_oracle.py work_dir
python voter.py work_dir
```
* Without Scenario Inference
``` bash
python -m ablation.get_oracle_candidates_no_scenario work_dir
python voter.py work_dir
python -m ablation.get_exception_judgement_no_scenario work_dir
```
* Without Both
``` bash
python -m ablation.get_oracle_candidates_no_scenario work_dir
python voter.py work_dir
```
