{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acdf294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "\n",
    "def get_eval_list(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    score_list = df[\"score\"].tolist()\n",
    "    return score_list\n",
    "\n",
    "rater1 = get_eval_list('evaluation_p1.csv')\n",
    "rater2 = get_eval_list('evaluation_p2.csv')\n",
    "rater3 = get_eval_list('evaluation_p3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d683d1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICC(2,k) [Average, Absolute]: \n",
      "    ICC        CI95%\n",
      "0.83922 [0.81, 0.87] \n",
      "\n",
      "ICC(3,k) [Average, Consistency]: \n",
      "     ICC        CI95%\n",
      "0.839311 [0.81, 0.87]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'rater1': rater1,\n",
    "    'rater2': rater2,\n",
    "    'rater3': rater3\n",
    "})\n",
    "\n",
    "\n",
    "df_long = df.reset_index().melt(\n",
    "    id_vars='index',\n",
    "    var_name='rater',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "\n",
    "icc = pg.intraclass_corr(\n",
    "    data=df_long,\n",
    "    targets='index',   \n",
    "    raters='rater',    \n",
    "    ratings='score'    \n",
    ")\n",
    "\n",
    "# print(icc)\n",
    "\n",
    "icc2_single = icc.loc[icc['Type'] == 'ICC2', ['ICC', 'CI95%']]\n",
    "icc2_avg    = icc.loc[icc['Type'] == 'ICC2k', ['ICC', 'CI95%']]\n",
    "\n",
    "icc3_avg    = icc.loc[icc['Type'] == 'ICC3k', ['ICC', 'CI95%']]\n",
    "\n",
    "print(f\"ICC(2,k) [Average, Absolute]: \\n{icc2_avg.to_string(index=False)} \\n\")\n",
    "print(f\"ICC(3,k) [Average, Consistency]: \\n{icc3_avg.to_string(index=False)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "522ff615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1 vs R2: 0.628\n",
      "R1 vs R3: 0.663\n",
      "R2 vs R3: 0.614\n",
      "Average Quadratic Weighted Kappa: 0.6352757240106623\n"
     ]
    }
   ],
   "source": [
    "# Weighted Kappa\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "raters = [rater1, rater2, rater3]\n",
    "names = ['R1', 'R2', 'R3']\n",
    "\n",
    "kappas = []\n",
    "\n",
    "for (i, j) in itertools.combinations(range(3), 2):\n",
    "    k = cohen_kappa_score(\n",
    "        raters[i],\n",
    "        raters[j],\n",
    "        weights='quadratic'\n",
    "    )\n",
    "    kappas.append(k)\n",
    "    print(f\"{names[i]} vs {names[j]}: {k:.3f}\")\n",
    "\n",
    "print(\"Average Quadratic Weighted Kappa:\", sum(kappas) / len(kappas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ba965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- quadratic weighting ---\n",
      "Rater 1 vs Rater 2: 0.6285\n",
      "Rater 1 vs Rater 3: 0.6629\n",
      "Rater 2 vs Rater 3: 0.6145\n",
      "\n",
      ">> Average Quadratic Weighted Kappa: 0.6353\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "raters_data = [rater1, rater2, rater3]\n",
    "\n",
    "def calculate_average_weighted_kappa(raters_list, weights_type='quadratic'):\n",
    "\n",
    "    kappa_scores = []\n",
    "    pairs = list(combinations(range(len(raters_list)), 2)) \n",
    "    \n",
    "    print(f\"--- {weights_type} weighting ---\")\n",
    "    \n",
    "    for i, j in pairs:\n",
    "        k = cohen_kappa_score(raters_list[i], raters_list[j], weights=weights_type)\n",
    "        kappa_scores.append(k)\n",
    "        print(f\"Rater {i+1} vs Rater {j+1}: {k:.4f}\")\n",
    "    \n",
    "    avg_kappa = np.mean(kappa_scores)\n",
    "    return avg_kappa\n",
    "\n",
    "avg_quad_kappa = calculate_average_weighted_kappa(raters_data, weights_type='quadratic')\n",
    "print(f\"\\n>> Average Quadratic Weighted Kappa: {avg_quad_kappa:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
